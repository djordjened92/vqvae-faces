{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from torch import nn, optim\n",
    "from collections import OrderedDict\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import defaultdict, Counter\n",
    "from skimage import io, transform\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "SHUFFLE = True\n",
    "NUM_WORKERS = 0\n",
    "CKPT_PERIOD = 20\n",
    "LOGS_VQVAE_PATH = './logs/pixelcnn'\n",
    "CODE_DIM, CODE_SIZE = 256, 128\n",
    "MODEL_NAME = 'model_1'\n",
    "CKPT_VQVAE_PATH = f'./checkpoints/pixelcnn/{MODEL_NAME}'\n",
    "Path(CKPT_VQVAE_PATH).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceImagesDataset(Dataset):\n",
    "    def __init__(self, path_pattern, transform=None):\n",
    "        self.transform = transform\n",
    "        self.root_paths = glob.glob(path_pattern)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.root_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        image = io.imread(self.root_paths[idx]).astype('float32')\n",
    "        image = image / 255.\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, train_loader, optimizer, epoch, quiet, grad_clip=None):\n",
    "    model.train()\n",
    "\n",
    "    if not quiet:\n",
    "        pbar = tqdm(total=len(train_loader.dataset))\n",
    "    losses = OrderedDict()\n",
    "    for x in train_loader:\n",
    "        x = x.cuda()\n",
    "        out = model.loss(x)\n",
    "        optimizer.zero_grad()\n",
    "        out['loss'].backward()\n",
    "        if grad_clip:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        desc = f'Epoch {epoch}'\n",
    "        for k, v in out.items():\n",
    "            if k not in losses:\n",
    "                losses[k] = []\n",
    "            losses[k].append(v.item())\n",
    "            avg_loss = np.mean(losses[k][-50:])\n",
    "            desc += f', {k} {avg_loss:.4f}'\n",
    "\n",
    "        if not quiet:\n",
    "            pbar.set_description(desc)\n",
    "            pbar.update(x.shape[0])\n",
    "    if not quiet:\n",
    "        pbar.close()\n",
    "    return losses\n",
    "\n",
    "\n",
    "def eval_loss(model, data_loader, quiet):\n",
    "    model.eval()\n",
    "    total_losses = OrderedDict()\n",
    "    with torch.no_grad():\n",
    "        for x in data_loader:\n",
    "            x = x.cuda()\n",
    "            out = model.loss(x)\n",
    "            for k, v in out.items():\n",
    "                total_losses[k] = total_losses.get(k, 0) + v.item() * x.shape[0]\n",
    "\n",
    "        desc = 'Validation '\n",
    "        for k in total_losses.keys():\n",
    "            total_losses[k] /= len(data_loader.dataset)\n",
    "            desc += f', {k} {total_losses[k]:.4f}'\n",
    "        if not quiet:\n",
    "            print(desc)\n",
    "    return total_losses\n",
    "\n",
    "\n",
    "def train_epochs(model, train_loader, val_loader, train_args, tb_writer, quiet=False):\n",
    "    epochs, lr = train_args['epochs'], train_args['lr']\n",
    "    grad_clip = train_args.get('grad_clip', None)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = train(model, train_loader, optimizer, epoch, quiet, grad_clip)\n",
    "        val_loss = eval_loss(model, val_loader, quiet)\n",
    "        \n",
    "        # Save checkpoints\n",
    "        if epoch % CKPT_PERIOD == 0:\n",
    "            ckpt_path = f'{CKPT_VQVAE_PATH}/{MODEL_NAME}_{epoch}.pt'\n",
    "            torch.save({\n",
    "                        'epoch': epoch,\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        }, ckpt_path)\n",
    "        # Write losses to tensorboard\n",
    "        for k in train_loss.keys():\n",
    "            tb_writer.add_scalar(f'training/{k}', np.mean(train_loss[k]), epoch)\n",
    "            tb_writer.add_scalar(f'validation/{k}', val_loss[k], epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.BatchNorm2d(dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(dim, dim, 3, 1, 1),\n",
    "            nn.BatchNorm2d(dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(dim, dim, 1)\n",
    "        )\n",
    "      \n",
    "    def forward(self, x):\n",
    "        return x + self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.LayerNorm):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 3, 1).contiguous()\n",
    "        x_shape = x.shape\n",
    "        x = super().forward(x)\n",
    "        return x.permute(0, 3, 1, 2).contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskConv2d(nn.Conv2d):\n",
    "    def __init__(self, mask_type, *args, conditional_size=None, **kwargs):\n",
    "        assert mask_type == 'A' or mask_type == 'B'\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.register_buffer('mask', torch.zeros_like(self.weight))\n",
    "        self.create_mask(mask_type)\n",
    "\n",
    "        if conditional_size is not None:\n",
    "            self.cond_op = nn.Linear(conditional_size, self.out_channels)\n",
    "\n",
    "    def forward(self, input, cond=None):\n",
    "        out = F.conv2d(input, self.weight * self.mask, self.bias, self.stride,\n",
    "                       self.padding, self.dilation, self.groups)\n",
    "        if cond is not None:\n",
    "            cond = self.cond_op(cond)\n",
    "            out = out + cond.view(cond.shape[0], self.out_channels, 1, 1)\n",
    "        return out\n",
    "\n",
    "    def create_mask(self, mask_type):\n",
    "        k = self.kernel_size[0]\n",
    "        self.mask[:, :, :k // 2] = 1\n",
    "        self.mask[:, :, k // 2, :k // 2] = 1\n",
    "        if mask_type == 'B':\n",
    "            self.mask[:, :, k // 2, k // 2] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelCNNResBlock(nn.Module):\n",
    "    def __init__(self, dim, conditional_size=None):\n",
    "        super().__init__()\n",
    "        self.block = nn.ModuleList([\n",
    "            LayerNorm(dim),\n",
    "            nn.ReLU(),\n",
    "            MaskConv2d('B', dim, dim // 2, 1, conditional_size=conditional_size),\n",
    "            LayerNorm(dim // 2),\n",
    "            nn.ReLU(),\n",
    "            MaskConv2d('B', dim // 2, dim // 2, 3, padding=1, \n",
    "                       conditional_size=conditional_size),\n",
    "            LayerNorm(dim // 2),\n",
    "            nn.ReLU(),\n",
    "            MaskConv2d('B', dim // 2, dim, 1, conditional_size=conditional_size)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, cond=None):\n",
    "        out = x\n",
    "        for layer in self.block:\n",
    "            if isinstance(layer, MaskConv2d):\n",
    "                out = layer(out, cond=cond)\n",
    "            else:\n",
    "                out = layer(out)\n",
    "        return x + out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelCNN(nn.Module):\n",
    "    def __init__(self, input_shape, code_size, dim=256, n_layers=7,\n",
    "                 conditional_size=None):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(code_size, dim)\n",
    "        model = nn.ModuleList([MaskConv2d('A', dim, dim, 7, padding=3,\n",
    "                                          conditional_size=conditional_size),\n",
    "                               LayerNorm(dim), nn.ReLU()])\n",
    "        for _ in range(n_layers - 1):\n",
    "            model.append(PixelCNNResBlock(dim, conditional_size=conditional_size))\n",
    "        model.extend([LayerNorm(dim), nn.ReLU(), MaskConv2d('B', dim, 512, 1, conditional_size=conditional_size),\n",
    "                      nn.ReLU(), MaskConv2d('B', 512, code_size, 1,\n",
    "                                            conditional_size=conditional_size)])\n",
    "        self.net = model\n",
    "        self.input_shape = input_shape\n",
    "        self.code_size = code_size\n",
    "\n",
    "    def forward(self, x, cond=None):\n",
    "        print(self.embedding.weight.shape)\n",
    "        out = self.embedding(x).permute(0, 3, 1, 2).contiguous()\n",
    "        for layer in self.net:\n",
    "            if isinstance(layer, MaskConv2d) or isinstance(layer, PixelCNNResBlock):\n",
    "                out = layer(out, cond=cond)\n",
    "            else:\n",
    "                out = layer(out)\n",
    "        return out\n",
    "\n",
    "    def loss(self, x, cond=None):\n",
    "        out = self(x, cond=cond)\n",
    "        return OrderedDict(loss=F.cross_entropy(out, x))\n",
    "\n",
    "    def sample(self, n, cond=None):\n",
    "        samples = torch.zeros(n, *self.input_shape).long().cuda()\n",
    "        with torch.no_grad():\n",
    "            for r in range(self.input_shape[0]):\n",
    "                for c in range(self.input_shape[1]):\n",
    "                    logits = self(samples, cond=cond)[:, :, r, c]\n",
    "                    logits = F.softmax(logits, dim=1)\n",
    "                    samples[:, r, c] = torch.multinomial(logits, 1).squeeze(-1)\n",
    "        return samples"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
