{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import defaultdict, Counter\n",
    "from skimage import io, transform\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and validation datasets are same for both tasks, but test sets are different. It seems that in tri-subject task there are some duplicates in the test set, but in kinship dataset images seems there is no duplications.\n",
    "\n",
    "Additionally, we explore dimensions distributions over images in all 3 datasets for kinship task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train persons: 3021\n",
      "Train images: 15845\n",
      "Val persons: 966\n",
      "Val images: 5045\n",
      "Test images: 5226\n"
     ]
    }
   ],
   "source": [
    "person_path_train = 'kinship_ver_t1/train-faces/*/MID*'\n",
    "person_path_val = 'kinship_ver_t1/val-faces/*/MID*'\n",
    "image_paths_train = os.path.join(person_path_train, '*.jpg')\n",
    "image_paths_val = os.path.join(person_path_val, '*.jpg')\n",
    "image_paths_test = 'kinship_ver_t1/test-faces/*.jpg'\n",
    "\n",
    "t_persons_paths = glob.glob(person_path_train)\n",
    "t_images_paths = glob.glob(image_paths_train)\n",
    "\n",
    "v_persons_paths = glob.glob(person_path_val)\n",
    "v_images_paths = glob.glob(image_paths_val)\n",
    "\n",
    "test_images_paths = glob.glob(image_paths_test)\n",
    "\n",
    "print(f'Train persons: {len(t_persons_paths)}')\n",
    "print(f'Train images: {len(t_images_paths)}')\n",
    "print(f'Val persons: {len(v_persons_paths)}')\n",
    "print(f'Val images: {len(v_images_paths)}')\n",
    "print(f'Test images: {len(test_images_paths)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset - shape values: Counter({(124, 108, 3): 15839, (224, 224, 3): 6})\n",
      "\n",
      "\n",
      "Valid dataset - shape values: Counter({(124, 108, 3): 4995, (224, 224, 3): 50})\n",
      "\n",
      "\n",
      "Test dataset - shape values: Counter({(124, 108, 3): 5217, (224, 224, 3): 9})\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, images_paths in [('Train', t_images_paths), ('Valid', v_images_paths), ('Test', test_images_paths)]:\n",
    "    shapes = []\n",
    "    for img_path in images_paths:\n",
    "        img = io.imread(img_path)\n",
    "        shapes.append(img.shape)\n",
    "\n",
    "    sh_cnt = Counter(shapes)\n",
    "    print(f'{name} dataset - shape values: {sh_cnt}')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main part of all images are of shape 124x108, only 65 out of ~26k images have shape 224x224.\n",
    "So, we'll accept standard shape for resizing images in all dataset as 124x108."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceImagesDataset(Dataset):\n",
    "    def __init__(self, path_pattern, transform=None):\n",
    "        self.transform = transform\n",
    "        self.root_paths = glob.glob(path_pattern)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.root_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        image = io.imread(self.root_paths[idx])\n",
    "        image = image / 255.\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 15845\n",
      "Validation dataset size: 5045\n",
      "Test dataset size: 5226\n"
     ]
    }
   ],
   "source": [
    "IMAGE_HEIGHT = 124\n",
    "IMAGE_WIDTH = 108\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "    transforms.Resize((IMAGE_HEIGHT, IMAGE_WIDTH))]\n",
    ")\n",
    "\n",
    "train_dataset = FaceImagesDataset('kinship_ver_t1/train-faces/*/MID*/*.jpg', transform=transform)\n",
    "val_dataset = FaceImagesDataset('kinship_ver_t1/val-faces/*/MID*/*.jpg', transform=transform)\n",
    "test_dataset = FaceImagesDataset('kinship_ver_t1/test-faces/*.jpg', transform=transform)\n",
    "\n",
    "print(f'Train dataset size: {len(train_dataset)}')\n",
    "print(f'Validation dataset size: {len(val_dataset)}')\n",
    "print(f'Test dataset size: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "SHUFFLE = True\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=SHUFFLE)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=SHUFFLE)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=SHUFFLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show example of batch from DataLoader\n",
    "\n",
    "# Helper function to show a batch\n",
    "def show_landmarks_batch(sample_batched):\n",
    "    \"\"\"Show image with landmarks for a batch of samples.\"\"\"\n",
    "    batch_size = len(sample_batched)\n",
    "    im_size = sample_batched.size(2)\n",
    "    grid_border_size = 2\n",
    "\n",
    "    grid = utils.make_grid(sample_batched)\n",
    "    plt.imshow(grid.numpy().transpose((1, 2, 0)))\n",
    "\n",
    "for i_batch, sample_batched in enumerate(train_dataloader):\n",
    "    print(i_batch, sample_batched.size())\n",
    "\n",
    "    # observe 4th batch and stop.\n",
    "    if i_batch == 3:\n",
    "        plt.figure()\n",
    "        show_landmarks_batch(sample_batched)\n",
    "        plt.axis('off')\n",
    "        plt.ioff()\n",
    "        plt.show()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
